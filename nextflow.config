includeConfig 'config/slurm_job.config'

manifest {
  description = 'Short-read assembly pipeline'
  author = 'Jane Hawkey'
  nextflowVersion = '>=20.01.0'
}

params {
  // Input and Output
  //reads = '/scratch/js66/jane/superbugAi/blood/read_test/*.fastq.gz'
  //reads = ''
  //assemblies = ''
  assemblies = '/scratch/cn64/jane/superbug_prospective_cohort/hybracter_assembly_noMedaka_Oct2024/FINAL_OUTPUT/*/*_final.fasta'
  output_dir = '/scratch/cn64/jane/superbug_prospective_cohort/bluedog_Oct2024_hybracterAssemblies_All'

  // Options to run
  read_qc = false //set to true to run fastqc and multiqc on all read sets
  run_speciator = true //set to true to run the PathogenWatch species detection tool on all assemblies
  run_kleborate = false //set to true to run Kleborate on all assemblies
  run_mlst = true //set to true to run the mlst tool on all assemblies
  run_amr = true //set to true to run AMRFinderPlus on all assemblies
  amr_organism = '' //set to organism value specified by AMRFinderPlus if you'd like organism specific filtering, see https://github.com/ncbi/amr/wiki/Running-AMRFinderPlus#--organism-option for more details

  // Executor
  // Maximum retries before ignoring job
  max_retries = 3
  // Maximum jobs to submit to the SLURM queue at once
  queue_size = 1000
  // Number of processors to use for local execution
  processors = 4
  // Account for SLURM
  slurm_account = 'cn64'
}

profiles {
  standard {
    executor {
        name = 'local'
        queueSize = params.processors
    }
  }

  massive {
    executor {
      name = 'slurm'
      // Queue size
      queueSize = params.queue_size
      // Submission rate to 2 per second
      submitRateLimit = '2/1.seconds'
      // Modify job-name, replace 'nf' with 'rd'
      // For whatever reason I can't access task.processor.name here so instead we do
      // a string sub to achieve desired result (nextflow 20.04.1)
      jobName = { "bd-${task.name}".replace(' ', '_') }
  }

  process {
      beforeScript = 'module load singularity'
      // Retry jobs - typically with more resources
      // NOTE: referencing task.maxRetries in the errorStrategy closure causes a recursion loop in nf
      // during task resubmission and crashes due to a stack overflow; must be specified as done below
      maxRetries = params.max_retries
      errorStrategy = { task.attempt < params.max_retries ? 'retry' : 'ignore' }

      // Required for consistent resumes on massives NFS
      cache = 'lenient'

      // Set options absent from nf slurm api
      clusterOptions = {
        qos = ''
        partition = ''
        if (task.time <= 30.minutes) {
          qos = 'genomics'
          partition = 'comp,genomics,short'
        } else if (task.time <= 240.minutes) {
          qos = 'genomics'
          partition = 'comp,genomics'
        } else {
          qos = 'normal'
          partition = 'comp'
        }
        return "--account=${params.slurm_account} --qos=${qos} --partition=${partition}"
      }
    }
  }
}
