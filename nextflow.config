includeConfig 'config/slurm_job.config'

manifest {
  description = 'Short-read assembly pipeline'
  author = 'Jane Hawkey'
  nextflowVersion = '>=20.01.0'
}

params {
  // Input and Output
  //reads = '/scratch/js66/jane/bluedog_testing/example_reads/*.fastq.gz'
  reads = ''
  assemblies = '/scratch/js66/jane/bluedog_testing/example_fasta/*.fasta'
  output_dir = '/scratch/js66/jane/bluedog_testing/test_output'
  //run_info_dir = "${output_dir}/run_info/"

  // Options to run
  read_qc = false //set to true to run fastqc and multiqc on all read sets
  run_kleborate = true //set to true to run Kleborate on all assemblies
  run_mlst = true //set to true to run the mlst tool on all assemblies
  run_speciator = false //set to true to run the PathogenWatch species detection tool on all assemblies

  // Executor
  // Maximum retries before ignoring job
  max_retries = 3
  // Maximum jobs to submit to the SLURM queue at once
  queue_size = 1000
  // Number of processors to use for local execution
  processors = 4
  // Account for SLURM
  slurm_account = 'js66'
}

profiles {
  standard {
    executor {
        name = 'local'
        queueSize = params.processors
    }
  }

  massive {
    executor {
      name = 'slurm'
      // Queue size
      queueSize = params.queue_size
      // Submission rate to 2 per second
      submitRateLimit = '2/1.seconds'
      // Modify job-name, replace 'nf' with 'rd'
      // For whatever reason I can't access task.processor.name here so instead we do
      // a string sub to achieve desired result (nextflow 20.04.1)
      jobName = { "bd-${task.name}".replace(' ', '_') }
  }

  process {
      // Retry jobs - typically with more resources
      // NOTE: referencing task.maxRetries in the errorStrategy closure causes a recursion loop in nf
      // during task resubmission and crashes due to a stack overflow; must be specified as done below
      maxRetries = params.max_retries
      errorStrategy = { task.attempt < params.max_retries ? 'retry' : 'ignore' }

      // Required for consistent resumes on massives NFS
      cache = 'lenient'

      // Set options absent from nf slurm api
      clusterOptions = {
        qos = ''
        partition = ''
        if (task.time <= 30.minutes) {
          qos = 'genomics'
          partition = 'comp,genomics,short'
        } else if (task.time <= 240.minutes) {
          qos = 'genomics'
          partition = 'comp,genomics'
        } else {
          qos = 'normal'
          partition = 'comp'
        }
        return "--account=${params.slurm_account} --qos=${qos} --partition=${partition}"
      }
    }
  }
}
